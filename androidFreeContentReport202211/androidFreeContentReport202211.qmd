---
title: "Android Free Content and Features Experiment"
subtitle: "Results Analysis"
date: last-modified
date-format: "[Last Updated:] DD MMMM, YYYY"
author: "Cesaire Tobias"
format: PrettyPDF-pdf
---

```{r eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}

# optimizely results params
optimizely_results_path <- './inst/extdata/optimizely_export_20230109_20230317.csv'
reqd_metric_tbl <- tibble::tribble(
  ~ reqd_metric,                 ~reqd_type,
  'Project Exported',            'total conversions',
  'Project Exported',            'unique conversions',
  'D0 Second Project Exported',  'unique conversions',
  'Application Opened After D0', 'unique conversions',
  'Free Trial Upsell Converted', 'unique conversions') %>%
  tidyr::unite("metric_id", reqd_metric, reqd_type, sep = "-")

exp_variants <- c('control', 'treatment-7', 'treatment-14')

start_dt <- '2023-01-09' # experiment started
end_alloc_dt <- '2023-02-01' # stop allocating users to experiment
end_dt <- '2023-03-17' # results analysed at
pre_p_start <- '2022-11-09' # start pre period here for pre-post analysis

# bootstrap params
seed = 123 # set seed for reproducibility with sampling
b <- 10000  # number of bootstrap samples
num_var <- 'pct_users_retained' # numeric variable containing the values to test
group_var <- 'variant_name' # grouping variable - used to identify the variants we wish to test

```

```{r eval = TRUE, echo = FALSE}
# since the live BQ conn has had auth issues when rendering, read the data from an RDS file where a separate process is used to retrieve data from BQ
bq_raw_data <- readRDS('./inst/extdata/bq_data_exports.rds')

# Get data from BQ for retention testing
bq_ret_data <- dplyr::filter(bq_raw_data, name == "retention_data") %>% dplyr::select(value) %>% tidyr::unnest(value)

# Get activation data from BQ
bq_act_data <- dplyr::filter(bq_raw_data, name == "activation_data") %>% dplyr::select(value) %>% tidyr::unnest(value)

# Get CAD data from BQ
bq_cad_data <- dplyr::filter(bq_raw_data, name == "cad_data") %>% dplyr::select(value) %>% tidyr::unnest(value)

# Get pro content data from BQ
bq_pro_content_data <- dplyr::filter(bq_raw_data, name == "pro_content_data") %>% dplyr::select(value) %>% tidyr::unnest(value)

# Get data for revenue calculations from BQ
bq_revenue_data <- dplyr::filter(bq_raw_data, name == "revenue_data") %>% dplyr::select(value) %>% tidyr::unnest(value)

# Get data for pre-post engagement calcs from BQ
bq_exports_data <- dplyr::filter(bq_raw_data, name == "exports_data") %>% dplyr::select(value) %>% tidyr::unnest(value)
```

```{r echo = FALSE, message=FALSE, warning=FALSE}

# convert raw optimizely results extract to neatly formatted table of results

reqd_cols <- c('Variation Name', 'Metric Event Name', 'Metric Numerator Type', 'Metric Denominator Type', 'Numerator Value',	'Denominator Value', 'Metric Value', 'Improvement Value from Baseline',	'Statistical Significance', 'Confidence Interval - Low',	'Confidence Interval - High')

raw_optimizely <- readr::read_csv(optimizely_results_path)

clean_optimizely <- raw_optimizely %>%
  dplyr::select(dplyr::all_of(reqd_cols)) %>%
  dplyr::rename(variation_name = 1,
                metric = 2,
                metric_numerator_type = 3,
                metric_denominator_type = 4,
                numerator_value = 5,
                denominator_value = 6,
                value = 7,
                change_vs_baseline = 8,
                statsig = 9,
                ci_low = 10,
                ci_high = 11) %>%
  dplyr::mutate(variation_name = stringr::str_remove_all(variation_name, '"'),
                metric = stringr::str_remove_all(metric, '"'),
                metric = stringr::str_remove_all(metric, '"')) %>%
  dplyr::mutate(change_vs_baseline = as.numeric(change_vs_baseline) * 100,
                statsig = as.numeric(statsig) * 100,
                ci_low = as.numeric(ci_low) * 100,
                ci_high = as.numeric(ci_high) * 100
  )
```

```{r eval = TRUE, echo = FALSE}
ret_act_df <- bq_act_data %>% 
  dplyr::arrange(variant_name, dplyr::desc(max_nday)) %>% 
  dplyr::group_by(variant_name, max_nday) %>%
  dplyr::mutate(users = dplyr::n_distinct(user_id),
                activations = dplyr::n_distinct(first_project_export_time)
  ) %>% 
  dplyr::ungroup() %>% 
  dplyr::group_by(variant_name) %>% 
  dplyr::mutate(total_users_per_variant = dplyr::n_distinct(user_id),
                total_activations_per_variant = dplyr::n_distinct(first_project_export_time)
                ) %>% 
  dplyr::distinct(max_nday, .keep_all = TRUE) %>% 
  dplyr::mutate(users_retained = cumsum(users),
                users_activated = cumsum(activations)
                ) %>% 
  dplyr::arrange(variant_name, max_nday) %>% 
  dplyr::mutate(retained_pct = users_retained / total_users_per_variant,
                activated_pct = users_activated / total_activations_per_variant
                ) %>%
  dplyr::ungroup() %>% 
  dplyr::select(variant_name,
                nday = max_nday,
                users,
                total_users_per_variant,
                users_retained,
                retained_pct,
                activations,
                total_activations_per_variant,
                users_activated,
                activated_pct
                )
```

```{r eval = TRUE, echo = FALSE}
run_bootstrap <- function(bq_data, exp_variants, num_var, group_var, b_samples, seed = NULL) {

  if(!is.null(seed)){
    set.seed(seed)
  }

  df <- bq_data %>%
    dplyr::select(!!group_var, !!num_var) %>%
    dplyr::arrange(dplyr::across(1)) %>%
    as.data.frame()

  levels(df[, group_var]) <- c(exp_variants[1], exp_variants[2])

  #  diff in means
  mean_variants <- with(df, tapply(df[, num_var], df[, group_var], mean))
  mean_diff <- mean_variants[1] - mean_variants[2]
  diff_mean <- (mean_diff)

  #  diff in medians
  median_variants <- with(df, tapply(df[, num_var], df[, group_var], median))
  median_diff <- median_variants[1] - median_variants[2]
  diff_median <- (median_diff)

  t_test_form <- as.formula(paste0(num_var,"~", group_var))
  t_test_pub <- rstatix::t_test(df, t_test_form, conf.level = 0.9, detailed = TRUE) # neatly format t-test results

  #############################  Bootstrapping


  n <- length(df[, group_var])  # number of observations to sample
  resample_var <- df[, num_var]  # variable to resample from

  BootstrapSamples <- matrix(
    sample(resample_var, size = n*b_samples, replace = TRUE),
    nrow = n, ncol = b_samples)

  # initialize the vector to store the test-stats
  boot_diff_mean <- rep(0, b_samples)
  boot_diff_median <- rep(0, b_samples)

  obs <- table(df[, group_var])
  length_var1 <- obs[[1]]
  length_var2 <- obs[[2]]
  total_length <- length_var1 + length_var2

  for (i in 1:b_samples) {
    boot_diff_mean[i] <- (mean(BootstrapSamples[1:length_var1, i]) - mean(BootstrapSamples[(length_var1+1):total_length, i]))
    boot_diff_median[i] <- (median(BootstrapSamples[1:length_var1, i]) - median(BootstrapSamples[(length_var1+1):total_length, i]))
  }

  # achieve significance levels
  asl_mean <- mean(boot_diff_mean >= diff_mean)
  asl_median <- mean(boot_diff_median >= diff_median)
  
  boot_df <- data.frame("boot_diff_mean" = boot_diff_mean,
                        "boot_diff_median" = boot_diff_median)

  result_list <- list (data = df,
                       diff_mean = diff_mean,
                       diff_median = diff_median,
                       asl_mean = asl_mean,
                       asl_median = asl_median,
                       bootstrapped_data = boot_df,
                       t_results = t_test_pub
                       # wilcox_results = wilcox_test,
                       # ks_results = ks_test
                       )

  result <- tibble::enframe(result_list)
  result
}
```

```{r eval = TRUE, echo = FALSE}
# Run bootstrap
# the bootstrapping algo only supports 2 variants at a time

exp_variants_1 <- c(exp_variants[1], exp_variants[2]) # control vs treatment 1
bs_req_data_1 <- dplyr::filter(bq_ret_data, variant_name %in% exp_variants_1)

bootstrap_data_set1 <- run_bootstrap(bs_req_data_1, 
                                     exp_variants = exp_variants_1,
                                     num_var, 
                                     group_var, 
                                     b_samples = b, 
                                     seed = seed)

exp_variants_2 <- c(exp_variants[1], exp_variants[3]) # control vs treatment 2
bs_req_data_2 <- dplyr::filter(bq_ret_data, variant_name %in% exp_variants_2)

bootstrap_data_set2 <- run_bootstrap(bs_req_data_2, 
                                     exp_variants = exp_variants_2,
                                     num_var, 
                                     group_var, 
                                     b_samples = b, 
                                     seed = seed)
```

## Background

The Android Free content and features experiment was designed to test whether a period of unrestricted usage would be an effective means of activating new users. 'Features' refers to, remove background, ability to upload custom logo, fonts, colour palettes etc while 'Content' refers to, templates, images, videos, graphics, fonts, articles, shapes etc. Note that users where not required to opt-in to join the treatment groups. Three variants were used, `r exp_variants[[1]]`, `r exp_variants[[2]]` (7 days free) and `r exp_variants[[3]]` (14 days free). The following analysis provides a summary of findings with further details provided for the maximum n day retention rates observed in the experiment participants. For more details on the experiment, see the proposal [here](https://godaddy-corp.atlassian.net/wiki/spaces/MV/pages/52660745/Free+content+and+features+experiment+for+new+users). For details, on the experiment outcomes for other metrics, see the Optimizely results [here](https://app.optimizely.com/v2/projects/14410340138/results/22576171398/experiments/22575942165?previousView=EXPERIMENTS).

## Experiment Details

The experiment allocated new users on Android to the treatment groups (`r exp_variants[[2]]` and `r exp_variants[[3]]`) and `r exp_variants[[1]]` for `r as.numeric(round(difftime(end_alloc_dt, start_dt, units = "weeks"),0))` weeks between `r format(as.Date(start_dt), "%d %B %Y")` and `r format(as.Date(end_alloc_dt), "%d %B %Y")`. New user allocation was then reduced to 0% while the experiment participants were tracked for a further `r as.numeric(round(difftime(end_dt, end_alloc_dt, units = "weeks"),0))` weeks to `r format(as.Date(end_dt), "%d %B %Y")`, in order to assess retention rates. The analysis makes use of maximum n day retention.

```{r echo=FALSE}
ss_data <- dplyr::select(clean_optimizely, variation_name, denominator_value) %>% dplyr::group_by(variation_name) %>% dplyr::distinct() %>% dplyr::ungroup()
var1 <- dplyr::filter(ss_data, variation_name == exp_variants[[1]]) %$% denominator_value
var2 <- dplyr::filter(ss_data, variation_name == exp_variants[[2]]) %$% denominator_value
var3 <- dplyr::filter(ss_data, variation_name == exp_variants[[3]]) %$% denominator_value
```

sample size (n) = `r format(sum(ss_data$denominator_value), big.mark = ',')`

-   `r exp_variants[[1]]`: `r format(var1, big.mark = ',')` `r paste0('(', round(var1 / sum(ss_data$denominator_value) * 100, 2), "%)")`

-   `r exp_variants[[2]]`: `r format(var2, big.mark = ',')` `r paste0('(', round(var2 / sum(ss_data$denominator_value) * 100, 2), "%)")`

-   `r exp_variants[[3]]`: `r format(var3, big.mark = ',')` `r paste0('(', round(var3 / sum(ss_data$denominator_value) * 100, 2), "%)")`

## Summary Results

```{r echo = FALSE, warning = FALSE, message = FALSE}
  
  # prepare the tidied optimizely data for table
    pub_tbl <- clean_optimizely %>%
      tidyr::unite("metric_id", metric, metric_numerator_type, sep = "-", remove = FALSE) %>%
      dplyr::right_join(reqd_metric_tbl, by = "metric_id") %>%
      dplyr::mutate(numerator_value = format(numerator_value, big.mark = ',')) %>% 
      dplyr::mutate(value = ifelse(metric_numerator_type == 'unique conversions', paste0(round(value * 100, 2), "%"), round(value, 2))) %>%
      dplyr::mutate(statsig = ifelse(statsig == 100, ">99%", paste0(statsig, "%"))) %>%
      dplyr::mutate(metric_id = stringr::str_replace_all(metric_id, "-total conversions", " (Avg)"),
                    metric_id = stringr::str_replace_all(metric_id, "-unique conversions", " (%)")
      ) %>%
      dplyr::mutate(conf_int = paste0("[", round(ci_low, 2),"%;", round(ci_high, 2),"%]"),
                    change_vs_baseline = paste0(round(change_vs_baseline, 2), "%")
      ) %>%
      dplyr::select(variation_name,
                    Metric = metric_id,
                    Conversions = numerator_value,
                    `Conversion Rate` = value,
                    Improvement = change_vs_baseline,
                    `Stat-Sig` = statsig,
                    `Conf int.` = conf_int)
```

```{r echo = FALSE, warning = FALSE, message = FALSE}
#| label: tbl-optimizely_results
#| tbl-cap: Optimizely Results
#| tbl-subcap:
#| - "Treatment-7 vs Control"
#| - "Treatment-14 vs Control"
#| layout-nrow: 2
pub_tbl %>% 
  dplyr::filter(variation_name == exp_variants[2]) %>% 
  dplyr::select(!variation_name) %>% 
knitr::kable()

pub_tbl %>% 
  dplyr::filter(variation_name == exp_variants[3]) %>% 
  dplyr::select(!variation_name) %>% 
knitr::kable()
```

In @tbl-optimizely_results, `Conversions` for % metrics are unique conversions while for avg metrics they are total conversions. The `Conversion Rate` is therefore, total conversions per user for avg metrics and unique conversions per user for % metrics. `Improvement`, represents the change in conversion rate relative to the control. We see statistically significant improvements in our engagement and activation metrics while upsell conversions significantly underperformed.

## Learnings

### Activation

We use `d0 project exports` as our activation metric. For more on activation, please see the knowledge repo document, [here](https://knowledge.mvdataserv.int.gdcorp.tools/posts/activation.html).

```{r, eval = TRUE, echo = FALSE}
activation_pie <- bq_act_data %>%
  dplyr::mutate(date = as.Date(h0)) %>%
  dplyr::filter(date <= end_alloc_dt) %>%
  dplyr::group_by(variant_name) %>%
  dplyr::mutate(activated_d0 = sum(!is.na(first_project_export_time)) / dplyr::n(),
                not_activated = 1 - activated_d0) %>%
  dplyr::ungroup() %>%
  dplyr::select(variant_name, activated_d0, not_activated) %>%
  dplyr::distinct() %>% 
  tidyr::pivot_longer(cols = c("activated_d0", "not_activated"), names_to = "type", values_to = "value") %>% 
  dplyr::mutate(value = round(value*100,2))
```

```{r, eval = TRUE, echo = FALSE}
activation_line <- bq_act_data %>%
  dplyr::mutate(date = as.Date(h0)) %>%
  dplyr::group_by(variant_name, date) %>%
  dplyr::mutate(activated_d0_first_pct = sum(!is.na(first_project_export_time)) / dplyr::n()) %>%
  dplyr::ungroup() %>%
  dplyr::select(variant_name, date, activated_d0_first_pct) %>%
  dplyr::distinct() %>%
  dplyr::filter(date <= end_alloc_dt)

act_avg_diff <- activation_line %>% 
  dplyr::group_by(variant_name) %>% 
  dplyr::summarise(avg = mean(activated_d0_first_pct)) %>% 
  dplyr::ungroup() %>% 
  tidyr::pivot_wider(names_from = variant_name, values_from = avg) %>% 
  dplyr::mutate(diff = `treatment-7` - control) %$%
  round(diff * 100,0)
```

@fig-activation shows the activation rates as measured by `d0 first project exported`. While we see higher activation rates of \~`r act_avg_diff`ppt for the treatment groups relative to the control, there is little difference between `r exp_variants[[2]]` and `r exp_variants[[3]]`.

```{r, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
#| label: fig-activation
#| fig-cap: "Activation Rates"
#| fig-subcap:
#| - "Total activation rates"
#| - "Activation rate time series"
#| fig-pos: "h"
#| layout-nrow: 2
ggplot2::ggplot(activation_pie, ggplot2::aes(x = "", y = value, fill = type)) +
  ggplot2::geom_col(color = "black") +
  ggplot2::geom_text(ggplot2::aes(label = paste0(value, "%")),
            position = ggplot2::position_stack(vjust = 0.5)) +
  ggplot2::coord_polar(theta = "y") +
  ggplot2::scale_fill_brewer() +
  ggplot2::theme_void() +
  ggplot2::labs(title = "Total Activation Rates",
                subtitle = paste0(start_dt, " - ", end_alloc_dt)
                ) +
  ggplot2::facet_wrap(~variant_name) + 
  ggplot2::theme(legend.position = "bottom")

ggplot2::ggplot(activation_line, ggplot2::aes(x = date, y = activated_d0_first_pct, color = variant_name))+
  ggplot2::geom_line()+
  ggplot2::theme_minimal()+
  ggplot2::labs(x = "Date",
                y = "Activation Rate (%)",
                title = "Activation Rates per Variant",
                subtitle = "D0 First Project Exported")+
  ggplot2::theme(legend.position = "bottom")
```

### Engagement

Looking at user engagement, we see in @fig-weekly_cad that canvas active days is higher in the treatment groups up until \~10 days after new users stopped being allocated to the experiment. For a more detailed view on measuring engagement, see the [User Engagement](https://knowledge.mvdataserv.int.gdcorp.tools/posts/user-engagement.html) and [Engagement Metrics](https://knowledge.mvdataserv.int.gdcorp.tools/posts/engagement-metrics.html) knowledge repo documents.

```{r eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
#| label: fig-weekly_cad
#| fig-cap: "Weekly average Canvas Active Days"
#| fig-pos: "h"

cad_data <- bq_cad_data %>% 
  tidyr::drop_na() %>% 
  dplyr::filter(sum_cad != users) %>% 
  dplyr::filter(date <= end_dt)

ggplot2::ggplot(cad_data, ggplot2::aes(x = date, y = weekly_avg_cad, color = variant_name)) +
  ggplot2::geom_line() +
  ggplot2::theme_minimal() +
  ggplot2::labs(x= "Date",
                y = "Avg CAD",
                title = "Weekly Average Canvas Active Days") +
  ggplot2::theme(legend.position = "bottom") + 
  ggplot2::geom_vline(ggplot2::aes(xintercept = as.Date(end_alloc_dt)),
                      linetype = "dashed", linewidth = 1)
```

As expected, we observe higher usage of the pro content elements.

```{r, echo = FALSE, warning = FALSE, message = FALSE}

#| label: fig-pro_content_taps

ggplot2::ggplot(bq_pro_content_data, ggplot2::aes(x = element_type, y = total_taps)) +
  ggplot2::geom_col(ggplot2::aes(color = variant_name, fill = variant_name), position =   ggplot2::position_dodge(0.8), width = 0.7) +
  ggplot2::theme_minimal() +
  ggplot2::labs(title = "Pro Content Taps") +
  ggplot2::theme(legend.position = "bottom")
```

@fig-pre_post shows new user average project exports before and after starting the experiment in an attempt to measure the impact of the intervention on engagement. The analysis is based on the work presented in [Pre-Post Experiment Analysis](https://secureservernet.sharepoint.com/:p:/r/teams/USIAnalytics/_layouts/15/Doc.aspx?sourcedoc=%7B414779FE-CC98-4227-BF14-0EEF040DEEAC%7D&file=Prepost%20Analysis%20Presentation.pptx&action=edit&mobileredirect=true).

```{r, echo = FALSE}

reqd_export_data <- bq_exports_data %>%
  dplyr::filter(user_type == 'new') %>%
  dplyr::arrange(date) %>%
  dplyr::filter(date >= pre_p_start & date <= end_dt) %>%
  dplyr::select(date, exports_avg, users)

reqd_export_data_aa <- bq_exports_data %>%
  dplyr::filter(user_type == 'new') %>%
  dplyr::arrange(date) %>%
  dplyr::filter(date >= pre_p_start & date <= start_dt) %>%
  dplyr::select(date, exports_avg, users)

exports_xts <- xts::as.xts(reqd_export_data)
exports_xts_aa <- xts::as.xts(reqd_export_data_aa)

post_p <- as.numeric(difftime(as.Date(end_dt), as.Date(start_dt)+1))
pre_p <-  as.numeric(difftime(as.Date(start_dt), as.Date(pre_p_start)))
p_ratio <- post_p / (post_p + pre_p)

# Set pre-period
pre_period <- c(as.Date(pre_p_start), as.Date(start_dt)-1)
pre_period_ind <- c(which(zoo::index(exports_xts) == pre_period[1]), which(zoo::index(exports_xts) == pre_period[2]))

pre_period_ind_aa <- c(which(zoo::index(exports_xts) == pre_period[1]), which(zoo::index(exports_xts) == pre_period[2]) * (1 - p_ratio))
pre_period_aa <- zoo::index(exports_xts[pre_period_ind_aa])

# Set post-period
post_period <- c(as.Date(start_dt), as.Date(end_dt))
post_period_ind <- c(which(zoo::index(exports_xts) == post_period[1]), which(zoo::index(exports_xts) == post_period[2]))

post_period_ind_aa <- c(which(zoo::index(exports_xts) == pre_period[2]) * (1 - p_ratio) + 1, which(zoo::index(exports_xts) == post_period[1]))
post_period_aa <- zoo::index(exports_xts[post_period_ind_aa])

# Calculate the pre-daily average
pre_daily_avg <- mean(exports_xts$exports_avg[pre_period_ind[1]:pre_period_ind[2]])
# Calculate the post-daily average
post_daily_avg <- mean(exports_xts$exports_avg[post_period_ind[1]:post_period_ind[2]])
# Pre-post difference
pre_post_diff <- post_daily_avg - pre_daily_avg

# Causal impact model
impact_aa <- CausalImpact::CausalImpact(data = exports_xts_aa, pre.period = pre_period_aa, post.period = post_period_aa)
impact <- CausalImpact::CausalImpact(data = exports_xts, pre.period = pre_period, post.period = post_period)
```

```{r echo=FALSE}
#| label: fig-pre_post
#| fig-cap: "Causal Impact Analysis"
#| fig-subcap:
#| - "AA Test"
#| - "Pre-Post Results"
#| fig-pos: "h"
#| layout-nrow: 2

# Pre/Post Visualization
ggplotify::as.ggplot(plot(impact_aa))
ggplotify::as.ggplot(plot(impact))

```

### Revenue

An important consideration when giving away free access is to assess the revenue implications. `monthly recurring revenue` (MRR) is used as the metric. A similar methodology used in [previous Black Friday experiments](https://secureservernet.sharepoint.com/teams/USIAnalytics/SitePages/Black-Friday-2020.aspx) has been used.

```{r eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
exp_start_dt <- start_dt

conv_pct <- bq_revenue_data %>% 
  dplyr::filter(date < exp_start_dt) %>% 
  dplyr::mutate(conv_pct_vec = actual_annual_conv / new_users) %>% 
  dplyr::summarise(avg = mean(conv_pct_vec, na.rm = TRUE)) %$%
  avg

sub_pct <- bq_revenue_data %>% 
  dplyr::filter(date < exp_start_dt) %>% 
  dplyr::mutate(sub_pct_vec = actual_monthly_subs / new_users) %>% 
  dplyr::summarise(avg = mean(sub_pct_vec, na.rm = TRUE)) %$%
  avg

monthly_mrr_sub <- bq_revenue_data %>% 
  dplyr::filter(date < exp_start_dt) %>% 
  dplyr::mutate(mrr_sub = mrr_monthly / subs_monthly) %>% 
  dplyr::summarise(avg = mean(mrr_sub, na.rm = TRUE)) %$% 
  avg

annual_mrr_sub <- bq_revenue_data %>% 
  dplyr::filter(date < exp_start_dt) %>% 
  dplyr::mutate(mrr_sub = mrr_annual / subs_annual) %>% 
  dplyr::summarise(avg = mean(mrr_sub, na.rm = TRUE)) %$% 
  avg

conv_rate <- bq_revenue_data %>%
  dplyr::filter(date < exp_start_dt) %>% 
  dplyr::summarise(avg = mean(conversion_rate, na.rm = TRUE)) %$% 
  avg

extrapolated_subs <- bq_revenue_data %>% 
  dplyr::select(date, new_users) %>% 
  dplyr::mutate(extrap_monthly_subs = new_users * sub_pct,
                extrap_monthly_mrr = extrap_monthly_subs * monthly_mrr_sub,
                extrap_annual_subs = new_users * conv_pct * conv_rate,
                extrap_annual_mrr = extrap_annual_subs * annual_mrr_sub)

mrr_tbl <- bq_revenue_data %>% 
  dplyr::select(date, actual_monthly_subs, actual_annual_conv) %>% 
  dplyr::mutate(monthly_mrr = actual_monthly_subs * monthly_mrr_sub,
                annual_mrr = actual_annual_conv * annual_mrr_sub)

exp_tbl <- bq_revenue_data %>% 
  dplyr::select(date, dplyr::contains("exp"))

monthly_revenue_tbl <- extrapolated_subs %>% 
  dplyr::left_join(mrr_tbl, by = "date") %>% 
  dplyr::left_join(exp_tbl, by = "date") %>% 
  tidyr::pivot_longer(
    cols = tidyr::contains("monthly"),
    names_to = "variable",
    values_to = "value",
    values_drop_na = TRUE
  ) %>% 
  dplyr::select(date, new_users, variable, value) %>% 
  dplyr::mutate(freq = "monthly")

annual_revenue_tbl <- extrapolated_subs %>% 
  dplyr::left_join(mrr_tbl, by = "date") %>% 
  dplyr::left_join(exp_tbl, by = "date") %>% 
  tidyr::pivot_longer(
    cols = tidyr::contains("annual"),
    names_to = "variable",
    values_to = "value",
    values_drop_na = TRUE
  ) %>% 
  dplyr::select(date, new_users, variable, value) %>% 
  dplyr::mutate(freq = "annual")

potential_revenue_monthly <- monthly_revenue_tbl %>% 
  dplyr::filter(freq == "monthly") %>% 
  dplyr::bind_rows(annual_revenue_tbl) %>% 
  dplyr::mutate(variable = stringr::str_remove_all(variable, "monthly_")) %>% 
  tidyr::pivot_wider(names_from = variable, values_from = value) %>% 
  dplyr::summarise(potential_subscribers = sum(actual_subs - extrap_subs, na.rm = TRUE),
                   potential_mrr = sum(mrr - extrap_mrr, na.rm = TRUE)
                   ) %>% 
  dplyr::mutate(period = "monthly")
  
potential_revenue_annual <- monthly_revenue_tbl %>% 
  dplyr::filter(freq == "annual") %>% 
  dplyr::bind_rows(annual_revenue_tbl) %>% 
  dplyr::mutate(variable = stringr::str_remove_all(variable, "annual_")) %>% 
  tidyr::pivot_wider(names_from = variable, values_from = value) %>% 
  dplyr::summarise(potential_subscribers = sum(actual_conv - extrap_subs, na.rm = TRUE),
                   potential_mrr = sum(mrr - extrap_mrr, na.rm = TRUE)
) %>% 
  dplyr::mutate(period = "annual")

potential_revenue_tbl <- potential_revenue_monthly %>% 
  dplyr::bind_rows(potential_revenue_annual) %>% 
  dplyr::select(period, potential_subscribers, potential_mrr)

exp_revenue_monthly <- exp_tbl %>% 
  dplyr::select(date, exp_subs_monthly, exp_monthly_subs, exp_mrr_monthly) %>% 
  dplyr::mutate(exp_mrr_sub = exp_mrr_monthly / exp_subs_monthly) %>% 
  dplyr::mutate(exp_mrr = exp_monthly_subs * exp_mrr_sub) %>% 
  dplyr::summarise(realised_subscribers = sum(exp_monthly_subs, na.rm = TRUE),
                   realised_mrr = sum(exp_mrr, na.rm = TRUE)
  ) %>% 
  dplyr::mutate(period = "monthly")

exp_revenue_annual <- exp_tbl %>% 
  dplyr::select(date, exp_subs_annual, exp_annual_conv, exp_mrr_annual) %>% 
  dplyr::mutate(exp_mrr_sub = exp_mrr_annual / exp_subs_annual) %>% 
  dplyr::mutate(exp_mrr = exp_annual_conv * exp_mrr_sub) %>% 
  dplyr::summarise(realised_subscribers = sum(exp_annual_conv, na.rm = TRUE),
                   realised_mrr = sum(exp_mrr, na.rm = TRUE)
  ) %>% 
  dplyr::mutate(period = "annual")

realised_revenue_tbl <- exp_revenue_monthly %>% 
  dplyr::bind_rows(exp_revenue_annual) %>% 
  dplyr::select(period, realised_subscribers, realised_mrr)

revenue_tbl <- potential_revenue_tbl %>% 
  dplyr::left_join(realised_revenue_tbl, by = "period") %>% 
  dplyr::select(period, realised_subscribers, potential_subscribers, realised_mrr, potential_mrr) %>% 
  dplyr::mutate(diff_subs_pct = (realised_subscribers / potential_subscribers - 1) * 100,
                diff_mrr_pct = (realised_mrr / potential_mrr - 1) * 100)
```

```{r echo = FALSE}
gt::gt(revenue_tbl) %>%
  gt::tab_header(
    title = "Revenue Results"
  ) %>% 
  gt::fmt_number(
    columns = 3:7,
    decimals = 1, 
    sep_mark = ","
    )
```

## Retention Analysis

M1 retention is the primary business success metric in this experiment. To evaluate M1 unbounded retention[^1] for new users on Android we refer to the techniques proposed in [Experimentation: User Retention Testing](https://knowledge.mvdataserv.int.gdcorp.tools/posts/user-retention-testing.html). We will make use of visual inspection and bootstrap testing.

[^1]: Whether the user opened the app any time between day 30 and 59.

### Visual Inspection

```{r eval = TRUE, echo = FALSE}
retention_curve_data <- bq_ret_data %>%
  dplyr::select(variant_name, nday, pct_users_retained)

retention_diff_data <- retention_curve_data %>%
  tidyr::pivot_wider(names_from = variant_name, values_from = pct_users_retained) %>%
  dplyr::mutate(absolute_diff = !!dplyr::sym(exp_variants[2]) - !!dplyr::sym(exp_variants[1])) %>%
  dplyr::mutate(relative_diff = (!!dplyr::sym(exp_variants[2]) / !!dplyr::sym(exp_variants[1]) - 1) * 100) %>%
  dplyr::select(!dplyr::all_of(exp_variants)) %>%
  tidyr::pivot_longer(!nday, names_to = "diff_type", values_to = "diff_value")

  diff_data_abs <- dplyr::filter(retention_diff_data, diff_type == "absolute_diff")
  diff_data_rel <- dplyr::filter(retention_diff_data, diff_type == "relative_diff")
```

Any experiment with retention as a primary metric should use the `application opened after d0` metric. This allows us to use Optimizely's stats engine to test significance.

In the event that significance is detected, visual inspection of unbounded retention should follow in order to provide an indication of the nature of the retention, viz., quick convergence, potential convergence or uniform improvement. In this experiment we find that the treatment outperforms the control in percentage of users retained and that the relative difference remains fairly stable at an average difference of `r round(mean(diff_data_rel$diff_value), 2)`% (see @fig-retention_diff).

```{r eval = TRUE, echo = FALSE, warning = FALSE}
#| label: fig-retention_curves
#| fig-cap: "Retention Curves"

ggplot2::ggplot(retention_curve_data, ggplot2::aes(x = nday, y = pct_users_retained, group = variant_name)) +
    ggplot2::geom_line(ggplot2::aes(color = variant_name)) +
    ggplot2::theme_minimal() +
    ggplot2::labs(title = paste0("n Day Retention Curves"),
                  x = 'days',
                  y = 'users retained (%)'
                  ) +
    ggplot2::theme(legend.position = "bottom")
```

@fig-retention_curves shows that the treatment groups outperform the control, with little difference between `r exp_variants[[2]]` and `r exp_variants[[3]]`. Through visual inspection we can be confident that there is an increase in D0 retention (confirmed in Optimizely with the `application opened after d0` event).

Taking the visual inspection a step further, @fig-retention_diff quantifies the differences between the curves for the `r exp_variants[[1]]` and `r exp_variants[[2]]`. We see a relatively stable, positive difference which would indicate improved retention as a result of the treatment. In the next section we make use of bootstrapping as a more rigorous quantitative approach to our assessment.

```{r eval = TRUE, echo = FALSE, warning = FALSE}
#| label: fig-retention_diff
#| fig-cap: "Differences in user retention over time"
#| fig-subcap:
#| - "Absolute Difference"
#| - "Relative Difference"
#| layout-nrow: 2

  ggplot2::ggplot(diff_data_abs, ggplot2::aes(x = nday, y = diff_value)) +
    ggplot2::geom_line(color = scales::hue_pal()(2)[2]) +
    ggplot2::theme_minimal() +
    ggplot2::labs(title = paste0("Retention Curve Absolute Difference"),
                  x = 'days',
                  y = 'absolute difference (ppt)'
    )

  ggplot2::ggplot(diff_data_rel, ggplot2::aes(x = nday, y = diff_value)) +
    ggplot2::geom_line(color = scales::hue_pal()(2)[1]) +
    ggplot2::theme_minimal() +
    ggplot2::labs(title = paste0("Retention Curve Relative Difference"),
                  x = 'days',
                  y = 'relative difference (%)'
    )
```

### Bootstrapping

Bootstrapping is a procedure that re-samples a single dataset, in this instance, the sample collected over the first 3 weeks of the experiment. This single sample is treated as one of many random samples that the experiment could have collected. We construct a sampling distribution from the simulated datasets created by drawing repeatedly from our actual data (`r b` times in this analysis).

The process works as follows:

1.  We assume that there is no difference between the control and the treatment. Our null hypothesis would be to say that they \[control and treatment\] are from the same distribution.

2.  With this assumption, we can simulate the difference we'd see from random sub-samples of the control and treatment users and create a probability distribution from this.

3.  We then compare this to the actual difference we observed in the experiment. If it's sufficiently unlikely that the observed difference falls within the sampling distribution, then we reject the null hypothesis.

```{r, eval = TRUE, echo = FALSE, warning = FALSE}
#| label: fig-retention_distro_box
#| fig-cap: "Observed Retention Distribution"

boxplot_data <- dplyr::filter(bootstrap_data_set1, name == "data") %>% tidyr::unnest(value)

ggplot2::ggplot(boxplot_data, ggplot2::aes_string(x = group_var, y = num_var)) +
    ggplot2::geom_boxplot() +
    ggplot2::theme_minimal() +
    ggplot2::stat_summary(ggplot2::aes_string(y = num_var), fun = mean, geom = "point", shape = 20, size = 3, color = scales::hue_pal()(2)[1], fill = scales::hue_pal()(2)[1]) +
    ggplot2::labs(ylab = "Users Retained (%)",
                  xlab = "Variant",
                  title = "Observed Retention Distribution")
```

```{r echo = FALSE}
diff_mean <- dplyr::filter(bootstrap_data_set1, name == "diff_mean") %>% tidyr::unnest(value)
diff_mean <- round(diff_mean$value, 2)
```

```{r echo = FALSE}
asl_mean <- dplyr::filter(bootstrap_data_set1, name == "asl_mean") %>% tidyr::unnest(value)
asl_mean <- round(asl_mean$value, 2)
```

@fig-retention_distro_box illustrates the distribution of our collected sample for the control and `r exp_variants[2]`. We observe both an increase in the mean as well as a wider inter-quartile range in the treatment. The treatment mean is `r if(diff_mean<0){paste0(-diff_mean, " percentage points higher than the control.")} else {print0(diff_mean, " percentage points lower than the control.")}`

```{r, echo = FALSE}
#| label: tbl-t_results
#| tbl-cap: "T-test results"

dplyr::filter(bootstrap_data_set1, name == "t_results") %>% 
  tidyr::unnest(value) %>% 
  dplyr::select(!!dplyr::sym(exp_variants[1]) := estimate1, 
                !!dplyr::sym(exp_variants[2]) := estimate2, 
                diff = estimate, n1, n2, test_stat = statistic, p, conf.low, conf.high) %>% 
gt::gt() %>%
  gt::tab_header(
    title = "Two-sample T-test",
    subtitle = "alpha: 90%"
  ) %>% 
  gt::fmt_number(
    columns = 1:9,
    decimals = 2, 
    sep_mark = ","
  )
```

@fig-bootstrapped_distro shows the observed difference in the means of the `r exp_variants[[1]]` and `r exp_variants[2]` against the bootstrapped distribution. The Achieved Significance Level (ASL)[^2] is at `r asl_mean` and we see `r exp_variants[[2]]` outperforming the `r exp_variants[[1]]`.

[^2]: Indicates the percentage of the distribution that is more than the observed value.

```{r, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
#| label: fig-bootstrapped_distro
#| fig-cap: "Bootstrapped Distribution"

densityplot_data <- dplyr::filter(bootstrap_data_set1, name == "bootstrapped_data") %>% 
  tidyr::unnest(value)

t_test_pub <- dplyr::filter(bootstrap_data_set1, name == "t_results") %>% tidyr::unnest(value)

ggplot2::ggplot(densityplot_data, ggplot2::aes(x = boot_diff_mean)) +
    ggplot2::geom_histogram(ggplot2::aes(y = ggplot2::after_stat(density)), colour = "black", fill = "white") +
    ggplot2::theme_minimal() + 
    ggplot2::geom_density(alpha = 0.2, fill = "#00A4A7") +
    ggplot2::geom_vline(ggplot2::aes(xintercept = diff_mean),
                        color = scales::hue_pal()(2)[1], linetype = "dashed", linewidth = 1) +
    ggplot2::geom_text(label = "Observed difference",
                       x = diff_mean,
                       y = 0.2,
                       angle = 90,
                       vjust = 1) +
    ggplot2::labs(x = "Difference in Means (ppt)",
                  y = "Probability Density",
                  title = "Bootstrapped Difference in Retention",
                  subtitle = paste("ASL: ", asl_mean)
                  )
```

## Next Steps
